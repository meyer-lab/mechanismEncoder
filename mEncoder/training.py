import numpy as np
import pandas as pd
import os
import fides
import logging
import scipy.linalg as la
from collections import namedtuple

from .autoencoder import MechanisticAutoEncoder
from . import pretrain_dir

from pypesto.optimize import (
    FidesOptimizer, minimize, OptimizeOptions
)
from pypesto import Problem, Result
from pypesto.objective.aesara import AesaraObjective


basedir = os.path.dirname(os.path.dirname(__file__))
trace_path = os.path.join(basedir, 'traces')
TRACE_FILE_TEMPLATE = '{pathway}__{data}__{n_hidden}__{job}__{{id}}.csv'


def generate_pypesto_objective(
        ae: MechanisticAutoEncoder
) -> AesaraObjective:
    """
    Creates a pypesto objective function (this is the loss function) that
    needs to be minimized to train the respective autoencoder

    :param ae:
        Autoencoder that will be trained

    :returns:
        Objective function that needs to be minimized for training.
    """

    return AesaraObjective(
        ae.pypesto_subproblem.objective, ae.x, ae.model_pars
    )


def create_pypesto_problem(
        ae:  MechanisticAutoEncoder
) -> Problem:
    """
    Creates a pypesto problem that defines the optimization problem that
    needs to be solved for the training of the provided autoencoder

    :param ae:
        Autoencoder that will be trained

    :returns:
        Optimization problem that needs to be solved for training.
    """
    return Problem(
        objective=generate_pypesto_objective(ae),
        x_names=ae.x_names,
        lb=[-np.inf for _ in ae.x_names],
        ub=[np.inf for _ in ae.x_names],
    )


def train(ae: MechanisticAutoEncoder,
          samplestr: str,
          n_starts: int = 1,
          seed: int = 0) -> Result:
    """
    Trains the provided autoencoder by solving the optimization problem
    generated by :py:func:`create_pypesto_problem`

    :param ae:
        Autoencoder that will be trained
    :param n_starts:
        number of local starts that will be performed
    :param seed:
        random seed that will be used to generate the randomly sampled
        initial startpoints

    :returns:
        Pypesto optimization results.
    """
    pypesto_problem = create_pypesto_problem(ae)
    opt = FidesOptimizer(
        hessian_update=fides.HybridUpdate(),
        options={
            fides.Options.FATOL: 1e-6,
            fides.Options.XTOL: 1e-8,
            fides.Options.MAXTIME: 3600 * 10,
            fides.Options.MAXITER: 1e3,
            fides.Options.SUBSPACE_DIM: fides.SubSpaceDim.TWO,
        },
        verbose=logging.INFO
    )

    np.random.seed(seed)

    optimize_options = OptimizeOptions(
        startpoint_resample=False,
        allow_failed_starts=True,
    )

    pca_pretraining = os.path.join(
        'pretraining', f'{ae.pathway_name}__{ae.data_name}'
                       f'__{samplestr}__pca__{ae.n_hidden}__{seed}.csv'
    )
    pretraining = pd.read_csv(pca_pretraining, index_col=0)

    w = np.expand_dims(la.lstsq(ae.data, ae.data_pca)[0].flatten(), 1)

    pypesto_problem.x_guesses_full = \
        np.expand_dims(np.hstack([w[:, 0], pretraining.values[0, :]]), 1).T

    return minimize(
        pypesto_problem,
        opt,
        n_starts=n_starts,
        options=optimize_options,
    )


SAMPLES = dict()

SAMPLES['dream_cytof'] = [
    'c184A1', 'cBT20', 'cBT474', 'cBT549', 'cCAL148', 'cCAL851',
    'cCAL51', 'cDU4475', 'cEFM192A', 'cEVSAT', 'cHBL100', 'cHCC1187',
    'cHCC1395', 'cHCC1419', 'cHCC1500', 'cHCC1569', 'cHCC1599',
    'cHCC1937', 'cHCC1954', 'cHCC2157', 'cHCC2185', 'cHCC3153',
    'cHCC38', 'cHCC70', 'cHDQP1', 'cJIMT1', 'cMCF10A', 'cMCF10F',
    'cMCF7', 'cMDAMB134VI', 'cMDAMB157', 'cMDAMB175VII', 'cMDAMB361',
    'cMDAMB415', 'cMDAMB453', 'cMDAkb2', 'cMFM223', 'cMPE600', 'cMX1',
    'cOCUBM', 'cT47D', 'cUACC812', 'cUACC893', 'cZR7530'
]

Wildcards = namedtuple('Wildcards', ['data', 'split'])


def training_samples(wildcards):
    samples = SAMPLES[wildcards.data]
    split, n_splits = wildcards.split.split('_')
    split = int(split)
    n_splits = int(n_splits)
    n_samples = len(samples)
    return samples[:int(np.round(n_samples/n_splits*split))] + \
        samples[int(np.round(n_samples/n_splits*(split+1))):]


def test_samples(wildcards):
    samples = SAMPLES[wildcards.data]
    split, n_splits = wildcards.split.split('_')
    split = int(split)
    n_splits = int(n_splits)
    n_samples = len(samples)
    return samples[int(np.round(n_samples/n_splits*split)):
                   int(np.round(n_samples/n_splits*(split+1))):]


def pretraining_samples_fun(wildcards):
    return [
        os.path.join(
            pretrain_dir, f'{{model}}__{{data}}__{{model}}__{sample}.csv'
        )
        for sample in training_samples(wildcards)
    ]

